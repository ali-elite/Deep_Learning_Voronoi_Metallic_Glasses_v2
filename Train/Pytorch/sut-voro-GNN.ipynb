{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>type</th>\n",
       "      <th>1-neighbour id</th>\n",
       "      <th>1-neighbour type</th>\n",
       "      <th>1-neighbour x</th>\n",
       "      <th>1-neighbour y</th>\n",
       "      <th>1-neighbour z</th>\n",
       "      <th>2-neighbour id</th>\n",
       "      <th>...</th>\n",
       "      <th>19-neighbour z</th>\n",
       "      <th>20-neighbour id</th>\n",
       "      <th>20-neighbour type</th>\n",
       "      <th>20-neighbour x</th>\n",
       "      <th>20-neighbour y</th>\n",
       "      <th>20-neighbour z</th>\n",
       "      <th>3-faced</th>\n",
       "      <th>4-faced</th>\n",
       "      <th>5-faced</th>\n",
       "      <th>6-faced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.0291</td>\n",
       "      <td>54.9202</td>\n",
       "      <td>24.4540</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>26.7424</td>\n",
       "      <td>53.8812</td>\n",
       "      <td>22.4257</td>\n",
       "      <td>4907.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.9562</td>\n",
       "      <td>5137.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>28.6902</td>\n",
       "      <td>53.0777</td>\n",
       "      <td>28.2413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42.4982</td>\n",
       "      <td>36.7826</td>\n",
       "      <td>37.0707</td>\n",
       "      <td>1.60</td>\n",
       "      <td>955.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>44.2310</td>\n",
       "      <td>38.3539</td>\n",
       "      <td>35.8460</td>\n",
       "      <td>13271.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34.3374</td>\n",
       "      <td>2060.0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>43.7738</td>\n",
       "      <td>39.0042</td>\n",
       "      <td>33.2507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.3755</td>\n",
       "      <td>24.9085</td>\n",
       "      <td>17.9155</td>\n",
       "      <td>1.60</td>\n",
       "      <td>8071.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>43.8712</td>\n",
       "      <td>22.7444</td>\n",
       "      <td>16.3405</td>\n",
       "      <td>821.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0551</td>\n",
       "      <td>13268.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>45.1087</td>\n",
       "      <td>20.5074</td>\n",
       "      <td>16.6933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.9179</td>\n",
       "      <td>13.2666</td>\n",
       "      <td>54.6982</td>\n",
       "      <td>1.28</td>\n",
       "      <td>13237.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>51.7705</td>\n",
       "      <td>14.4300</td>\n",
       "      <td>53.9390</td>\n",
       "      <td>5603.0</td>\n",
       "      <td>...</td>\n",
       "      <td>55.7498</td>\n",
       "      <td>3508.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>54.2731</td>\n",
       "      <td>17.6069</td>\n",
       "      <td>52.2377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.7815</td>\n",
       "      <td>28.6099</td>\n",
       "      <td>25.9021</td>\n",
       "      <td>1.28</td>\n",
       "      <td>10123.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>31.8357</td>\n",
       "      <td>30.5916</td>\n",
       "      <td>25.3170</td>\n",
       "      <td>7373.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28.7109</td>\n",
       "      <td>515.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>34.4023</td>\n",
       "      <td>29.2311</td>\n",
       "      <td>28.4718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x        y        z  type  1-neighbour id  1-neighbour type  \\\n",
       "0  26.0291  54.9202  24.4540  1.28          1734.0              1.28   \n",
       "1  42.4982  36.7826  37.0707  1.60           955.0              1.28   \n",
       "2  44.3755  24.9085  17.9155  1.60          8071.0              1.28   \n",
       "3  53.9179  13.2666  54.6982  1.28         13237.0              1.28   \n",
       "4  30.7815  28.6099  25.9021  1.28         10123.0              1.28   \n",
       "\n",
       "   1-neighbour x  1-neighbour y  1-neighbour z  2-neighbour id  ...  \\\n",
       "0        26.7424        53.8812        22.4257          4907.0  ...   \n",
       "1        44.2310        38.3539        35.8460         13271.0  ...   \n",
       "2        43.8712        22.7444        16.3405           821.0  ...   \n",
       "3        51.7705        14.4300        53.9390          5603.0  ...   \n",
       "4        31.8357        30.5916        25.3170          7373.0  ...   \n",
       "\n",
       "   19-neighbour z  20-neighbour id  20-neighbour type  20-neighbour x  \\\n",
       "0         19.9562           5137.0               1.28         28.6902   \n",
       "1         34.3374           2060.0               1.60         43.7738   \n",
       "2         17.0551          13268.0               1.28         45.1087   \n",
       "3         55.7498           3508.0               1.28         54.2731   \n",
       "4         28.7109            515.0               1.28         34.4023   \n",
       "\n",
       "   20-neighbour y  20-neighbour z  3-faced  4-faced  5-faced  6-faced  \n",
       "0         53.0777         28.2413      0.0      1.0      3.0      4.0  \n",
       "1         39.0042         33.2507      0.0      0.0      1.0     10.0  \n",
       "2         20.5074         16.6933      0.0      1.0      1.0      9.0  \n",
       "3         17.6069         52.2377      0.0      1.0      3.0      3.0  \n",
       "4         29.2311         28.4718      0.0      0.0      0.0     12.0  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../Output/output_1.csv')\n",
    "\n",
    "\n",
    "df = df.drop(\"nb\", axis='columns')\n",
    "df = df.drop(\"id\", axis='columns')\n",
    "df = df.drop(\"1-faced\", axis='columns')\n",
    "df = df.drop(\"2-faced\", axis='columns')\n",
    "df = df.drop(\"7-faced\", axis='columns')\n",
    "df = df.drop(\"8-faced\", axis='columns')\n",
    "df = df.drop(\"9-faced\", axis='columns')\n",
    "df = df.drop(\"10-faced\", axis='columns')\n",
    "df = df.drop(\"11-faced\", axis='columns')\n",
    "df = df.drop(\"12-faced\", axis='columns')\n",
    "df = df.drop(\"13-faced\", axis='columns')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main-atom</th>\n",
       "      <th>neighbour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1733.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4906.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>9344.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>12133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5434.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   main-atom  neighbour\n",
       "0          0     1733.0\n",
       "1          0     4906.0\n",
       "2          0     9344.0\n",
       "3          0    12133.0\n",
       "4          0     5434.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_df = []\n",
    "for index, row in df.iterrows():\n",
    "    for i in row.keys():\n",
    "        if 'neighbour id' in i:\n",
    "            edge_df.append([index, row[i]-1])\n",
    "edge_df = pd.DataFrame(edge_df, columns=['main-atom','neighbour'])\n",
    "\n",
    "edge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0.,     0.,     0., ..., 13499., 13499., 13499.],\n",
       "       [ 1733.,  4906.,  9344., ..., 10721., 10716.,  8260.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = edge_df[['main-atom','neighbour']].values.transpose()\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0.,     0.,     0.,  ..., 13499., 13499., 13499.],\n",
       "        [ 1733.,  4906.,  9344.,  ..., 10721., 10716.,  8260.]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.from_numpy(edge_index[0])\n",
    "b = torch.from_numpy(edge_index[1])\n",
    "edge_index_torch = torch.stack([a, b])\n",
    "edge_index_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>type</th>\n",
       "      <th>1-neighbour id</th>\n",
       "      <th>1-neighbour type</th>\n",
       "      <th>1-neighbour x</th>\n",
       "      <th>1-neighbour y</th>\n",
       "      <th>1-neighbour z</th>\n",
       "      <th>2-neighbour id</th>\n",
       "      <th>...</th>\n",
       "      <th>19-neighbour id</th>\n",
       "      <th>19-neighbour type</th>\n",
       "      <th>19-neighbour x</th>\n",
       "      <th>19-neighbour y</th>\n",
       "      <th>19-neighbour z</th>\n",
       "      <th>20-neighbour id</th>\n",
       "      <th>20-neighbour type</th>\n",
       "      <th>20-neighbour x</th>\n",
       "      <th>20-neighbour y</th>\n",
       "      <th>20-neighbour z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.0291</td>\n",
       "      <td>54.9202</td>\n",
       "      <td>24.4540</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>26.7424</td>\n",
       "      <td>53.8812</td>\n",
       "      <td>22.4257</td>\n",
       "      <td>4907.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7263.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>27.4960</td>\n",
       "      <td>53.51380</td>\n",
       "      <td>19.9562</td>\n",
       "      <td>5137.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>28.6902</td>\n",
       "      <td>53.0777</td>\n",
       "      <td>28.2413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42.4982</td>\n",
       "      <td>36.7826</td>\n",
       "      <td>37.0707</td>\n",
       "      <td>1.60</td>\n",
       "      <td>955.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>44.2310</td>\n",
       "      <td>38.3539</td>\n",
       "      <td>35.8460</td>\n",
       "      <td>13271.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10933.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>45.6085</td>\n",
       "      <td>34.84750</td>\n",
       "      <td>34.3374</td>\n",
       "      <td>2060.0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>43.7738</td>\n",
       "      <td>39.0042</td>\n",
       "      <td>33.2507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.3755</td>\n",
       "      <td>24.9085</td>\n",
       "      <td>17.9155</td>\n",
       "      <td>1.60</td>\n",
       "      <td>8071.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>43.8712</td>\n",
       "      <td>22.7444</td>\n",
       "      <td>16.3405</td>\n",
       "      <td>821.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12864.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>47.2375</td>\n",
       "      <td>28.08540</td>\n",
       "      <td>17.0551</td>\n",
       "      <td>13268.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>45.1087</td>\n",
       "      <td>20.5074</td>\n",
       "      <td>16.6933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.9179</td>\n",
       "      <td>13.2666</td>\n",
       "      <td>54.6982</td>\n",
       "      <td>1.28</td>\n",
       "      <td>13237.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>51.7705</td>\n",
       "      <td>14.4300</td>\n",
       "      <td>53.9390</td>\n",
       "      <td>5603.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10511.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>51.1129</td>\n",
       "      <td>9.36614</td>\n",
       "      <td>55.7498</td>\n",
       "      <td>3508.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>54.2731</td>\n",
       "      <td>17.6069</td>\n",
       "      <td>52.2377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.7815</td>\n",
       "      <td>28.6099</td>\n",
       "      <td>25.9021</td>\n",
       "      <td>1.28</td>\n",
       "      <td>10123.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>31.8357</td>\n",
       "      <td>30.5916</td>\n",
       "      <td>25.3170</td>\n",
       "      <td>7373.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5587.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>27.6471</td>\n",
       "      <td>30.12860</td>\n",
       "      <td>28.7109</td>\n",
       "      <td>515.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>34.4023</td>\n",
       "      <td>29.2311</td>\n",
       "      <td>28.4718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x        y        z  type  1-neighbour id  1-neighbour type  \\\n",
       "0  26.0291  54.9202  24.4540  1.28          1734.0              1.28   \n",
       "1  42.4982  36.7826  37.0707  1.60           955.0              1.28   \n",
       "2  44.3755  24.9085  17.9155  1.60          8071.0              1.28   \n",
       "3  53.9179  13.2666  54.6982  1.28         13237.0              1.28   \n",
       "4  30.7815  28.6099  25.9021  1.28         10123.0              1.28   \n",
       "\n",
       "   1-neighbour x  1-neighbour y  1-neighbour z  2-neighbour id  ...  \\\n",
       "0        26.7424        53.8812        22.4257          4907.0  ...   \n",
       "1        44.2310        38.3539        35.8460         13271.0  ...   \n",
       "2        43.8712        22.7444        16.3405           821.0  ...   \n",
       "3        51.7705        14.4300        53.9390          5603.0  ...   \n",
       "4        31.8357        30.5916        25.3170          7373.0  ...   \n",
       "\n",
       "   19-neighbour id  19-neighbour type  19-neighbour x  19-neighbour y  \\\n",
       "0           7263.0               1.28         27.4960        53.51380   \n",
       "1          10933.0               1.28         45.6085        34.84750   \n",
       "2          12864.0               1.28         47.2375        28.08540   \n",
       "3          10511.0               1.28         51.1129         9.36614   \n",
       "4           5587.0               1.28         27.6471        30.12860   \n",
       "\n",
       "   19-neighbour z  20-neighbour id  20-neighbour type  20-neighbour x  \\\n",
       "0         19.9562           5137.0               1.28         28.6902   \n",
       "1         34.3374           2060.0               1.60         43.7738   \n",
       "2         17.0551          13268.0               1.28         45.1087   \n",
       "3         55.7498           3508.0               1.28         54.2731   \n",
       "4         28.7109            515.0               1.28         34.4023   \n",
       "\n",
       "   20-neighbour y  20-neighbour z  \n",
       "0         53.0777         28.2413  \n",
       "1         39.0042         33.2507  \n",
       "2         20.5074         16.6933  \n",
       "3         17.6069         52.2377  \n",
       "4         29.2311         28.4718  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features = df.drop(columns=[x for x in df.columns if 'faced' in x])\n",
    "node_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3-faced</th>\n",
       "      <th>4-faced</th>\n",
       "      <th>5-faced</th>\n",
       "      <th>6-faced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   3-faced  4-faced  5-faced  6-faced\n",
       "0      0.0      1.0      3.0      4.0\n",
       "1      0.0      0.0      1.0     10.0\n",
       "2      0.0      1.0      1.0      9.0\n",
       "3      0.0      1.0      3.0      3.0\n",
       "4      0.0      0.0      0.0     12.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df.drop(columns=[x for x in df.columns if 'faced' not in x])\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "data = Data(x=torch.from_numpy(node_features.to_numpy()).float(),\n",
    "            edge_index=edge_index_torch.type(torch.int64),\n",
    "            y=torch.from_numpy(labels.to_numpy()).float(),\n",
    "           )\n",
    "\n",
    "data_long = Data(x=torch.from_numpy(node_features.to_numpy()).long(),\n",
    "            edge_index=edge_index_torch.long(),\n",
    "            y=torch.from_numpy(labels.to_numpy()).long(),\n",
    "           )\n",
    "\n",
    "assert data.edge_index.max() < data.num_nodes\n",
    "assert data.edge_index.max() < data.x.size(0)\n",
    "\n",
    "assert data_long.edge_index.max() < data_long.num_nodes\n",
    "assert data_long.edge_index.max() < data_long.x.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(104, 32)\n",
      "  (conv2): GCNConv(32, 4)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(104, 32)\n",
    "        self.conv2 = GCNConv(32, 4)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 4565902.5000\n",
      "Epoch: 002, Loss: 704029.4375\n",
      "Epoch: 003, Loss: 1172700.7500\n",
      "Epoch: 004, Loss: 1472179.0000\n",
      "Epoch: 005, Loss: 1690747.3750\n",
      "Epoch: 006, Loss: 795554.5625\n",
      "Epoch: 007, Loss: 231327.7500\n",
      "Epoch: 008, Loss: 187936.6562\n",
      "Epoch: 009, Loss: 312893.9688\n",
      "Epoch: 010, Loss: 358072.7500\n",
      "Epoch: 011, Loss: 486843.6562\n",
      "Epoch: 012, Loss: 367389.7188\n",
      "Epoch: 013, Loss: 291430.4688\n",
      "Epoch: 014, Loss: 242647.2344\n",
      "Epoch: 015, Loss: 163652.1719\n",
      "Epoch: 016, Loss: 83134.5312\n",
      "Epoch: 017, Loss: 49273.5273\n",
      "Epoch: 018, Loss: 51675.8789\n",
      "Epoch: 019, Loss: 39074.1758\n",
      "Epoch: 020, Loss: 26254.2637\n",
      "Epoch: 021, Loss: 39111.7539\n",
      "Epoch: 022, Loss: 16355.2051\n",
      "Epoch: 023, Loss: 23167.8223\n",
      "Epoch: 024, Loss: 15230.2930\n",
      "Epoch: 025, Loss: 14853.6396\n",
      "Epoch: 026, Loss: 18915.1348\n",
      "Epoch: 027, Loss: 10940.8389\n",
      "Epoch: 028, Loss: 11237.3027\n",
      "Epoch: 029, Loss: 12456.6494\n",
      "Epoch: 030, Loss: 6730.3652\n",
      "Epoch: 031, Loss: 6026.7949\n",
      "Epoch: 032, Loss: 4236.8936\n",
      "Epoch: 033, Loss: 4358.6299\n",
      "Epoch: 034, Loss: 6089.4004\n",
      "Epoch: 035, Loss: 4099.4395\n",
      "Epoch: 036, Loss: 2109.7441\n",
      "Epoch: 037, Loss: 1749.3231\n",
      "Epoch: 038, Loss: 2488.9822\n",
      "Epoch: 039, Loss: 1064.7896\n",
      "Epoch: 040, Loss: 973.5113\n",
      "Epoch: 041, Loss: 525.4842\n",
      "Epoch: 042, Loss: 533.0994\n",
      "Epoch: 043, Loss: 417.1960\n",
      "Epoch: 044, Loss: 302.0757\n",
      "Epoch: 045, Loss: 314.5750\n",
      "Epoch: 046, Loss: 195.5946\n",
      "Epoch: 047, Loss: 219.5147\n",
      "Epoch: 048, Loss: 188.1498\n",
      "Epoch: 049, Loss: 146.7957\n",
      "Epoch: 050, Loss: 167.4805\n",
      "Epoch: 051, Loss: 106.5091\n",
      "Epoch: 052, Loss: 109.4837\n",
      "Epoch: 053, Loss: 100.6389\n",
      "Epoch: 054, Loss: 86.4337\n",
      "Epoch: 055, Loss: 92.3523\n",
      "Epoch: 056, Loss: 84.3464\n",
      "Epoch: 057, Loss: 66.4528\n",
      "Epoch: 058, Loss: 82.9596\n",
      "Epoch: 059, Loss: 80.3365\n",
      "Epoch: 060, Loss: 75.4526\n",
      "Epoch: 061, Loss: 58.2063\n",
      "Epoch: 062, Loss: 55.4091\n",
      "Epoch: 063, Loss: 59.8710\n",
      "Epoch: 064, Loss: 48.0488\n",
      "Epoch: 065, Loss: 59.6880\n",
      "Epoch: 066, Loss: 48.1634\n",
      "Epoch: 067, Loss: 47.4092\n",
      "Epoch: 068, Loss: 50.6900\n",
      "Epoch: 069, Loss: 53.8066\n",
      "Epoch: 070, Loss: 40.7620\n",
      "Epoch: 071, Loss: 41.6678\n",
      "Epoch: 072, Loss: 44.0663\n",
      "Epoch: 073, Loss: 43.4412\n",
      "Epoch: 074, Loss: 42.9931\n",
      "Epoch: 075, Loss: 36.9741\n",
      "Epoch: 076, Loss: 45.3342\n",
      "Epoch: 077, Loss: 36.6256\n",
      "Epoch: 078, Loss: 40.4184\n",
      "Epoch: 079, Loss: 38.3329\n",
      "Epoch: 080, Loss: 32.0086\n",
      "Epoch: 081, Loss: 34.1849\n",
      "Epoch: 082, Loss: 32.9312\n",
      "Epoch: 083, Loss: 31.2870\n",
      "Epoch: 084, Loss: 34.9195\n",
      "Epoch: 085, Loss: 36.7015\n",
      "Epoch: 086, Loss: 36.6302\n",
      "Epoch: 087, Loss: 32.4341\n",
      "Epoch: 088, Loss: 31.8501\n",
      "Epoch: 089, Loss: 32.7191\n",
      "Epoch: 090, Loss: 33.1293\n",
      "Epoch: 091, Loss: 28.1188\n",
      "Epoch: 092, Loss: 29.5134\n",
      "Epoch: 093, Loss: 29.9579\n",
      "Epoch: 094, Loss: 27.8515\n",
      "Epoch: 095, Loss: 29.6157\n",
      "Epoch: 096, Loss: 30.4287\n",
      "Epoch: 097, Loss: 27.4846\n",
      "Epoch: 098, Loss: 29.0306\n",
      "Epoch: 099, Loss: 30.4058\n",
      "Epoch: 100, Loss: 28.9967\n",
      "Epoch: 101, Loss: 32.2131\n",
      "Epoch: 102, Loss: 30.7445\n",
      "Epoch: 103, Loss: 32.0203\n",
      "Epoch: 104, Loss: 26.5082\n",
      "Epoch: 105, Loss: 29.0596\n",
      "Epoch: 106, Loss: 23.9311\n",
      "Epoch: 107, Loss: 31.1682\n",
      "Epoch: 108, Loss: 26.2987\n",
      "Epoch: 109, Loss: 27.2518\n",
      "Epoch: 110, Loss: 26.8865\n",
      "Epoch: 111, Loss: 29.5746\n",
      "Epoch: 112, Loss: 28.6942\n",
      "Epoch: 113, Loss: 26.7078\n",
      "Epoch: 114, Loss: 21.9188\n",
      "Epoch: 115, Loss: 24.4714\n",
      "Epoch: 116, Loss: 27.9454\n",
      "Epoch: 117, Loss: 23.5254\n",
      "Epoch: 118, Loss: 26.6638\n",
      "Epoch: 119, Loss: 23.0132\n",
      "Epoch: 120, Loss: 25.4561\n",
      "Epoch: 121, Loss: 27.5671\n",
      "Epoch: 122, Loss: 27.9378\n",
      "Epoch: 123, Loss: 25.2911\n",
      "Epoch: 124, Loss: 24.8785\n",
      "Epoch: 125, Loss: 28.0292\n",
      "Epoch: 126, Loss: 22.4935\n",
      "Epoch: 127, Loss: 30.1081\n",
      "Epoch: 128, Loss: 22.8425\n",
      "Epoch: 129, Loss: 21.6900\n",
      "Epoch: 130, Loss: 25.4085\n",
      "Epoch: 131, Loss: 28.5184\n",
      "Epoch: 132, Loss: 26.2637\n",
      "Epoch: 133, Loss: 23.2474\n",
      "Epoch: 134, Loss: 26.1040\n",
      "Epoch: 135, Loss: 24.8758\n",
      "Epoch: 136, Loss: 23.3734\n",
      "Epoch: 137, Loss: 22.2890\n",
      "Epoch: 138, Loss: 24.0380\n",
      "Epoch: 139, Loss: 24.6039\n",
      "Epoch: 140, Loss: 22.0282\n",
      "Epoch: 141, Loss: 21.8541\n",
      "Epoch: 142, Loss: 23.5043\n",
      "Epoch: 143, Loss: 21.5756\n",
      "Epoch: 144, Loss: 22.6437\n",
      "Epoch: 145, Loss: 23.5239\n",
      "Epoch: 146, Loss: 20.9429\n",
      "Epoch: 147, Loss: 19.9973\n",
      "Epoch: 148, Loss: 22.8986\n",
      "Epoch: 149, Loss: 26.3762\n",
      "Epoch: 150, Loss: 23.1545\n",
      "Epoch: 151, Loss: 23.9960\n",
      "Epoch: 152, Loss: 21.7314\n",
      "Epoch: 153, Loss: 21.2676\n",
      "Epoch: 154, Loss: 21.7297\n",
      "Epoch: 155, Loss: 23.5244\n",
      "Epoch: 156, Loss: 20.9834\n",
      "Epoch: 157, Loss: 19.6504\n",
      "Epoch: 158, Loss: 22.8385\n",
      "Epoch: 159, Loss: 22.1816\n",
      "Epoch: 160, Loss: 21.9545\n",
      "Epoch: 161, Loss: 20.8194\n",
      "Epoch: 162, Loss: 27.2882\n",
      "Epoch: 163, Loss: 19.6974\n",
      "Epoch: 164, Loss: 22.8534\n",
      "Epoch: 165, Loss: 22.5619\n",
      "Epoch: 166, Loss: 20.7056\n",
      "Epoch: 167, Loss: 18.9796\n",
      "Epoch: 168, Loss: 22.3354\n",
      "Epoch: 169, Loss: 20.5323\n",
      "Epoch: 170, Loss: 19.4030\n",
      "Epoch: 171, Loss: 21.2120\n",
      "Epoch: 172, Loss: 22.3587\n",
      "Epoch: 173, Loss: 20.6248\n",
      "Epoch: 174, Loss: 21.8465\n",
      "Epoch: 175, Loss: 19.8447\n",
      "Epoch: 176, Loss: 21.3336\n",
      "Epoch: 177, Loss: 20.8338\n",
      "Epoch: 178, Loss: 20.1449\n",
      "Epoch: 179, Loss: 20.9229\n",
      "Epoch: 180, Loss: 21.9650\n",
      "Epoch: 181, Loss: 20.2675\n",
      "Epoch: 182, Loss: 21.4457\n",
      "Epoch: 183, Loss: 18.2998\n",
      "Epoch: 184, Loss: 21.2792\n",
      "Epoch: 185, Loss: 23.0605\n",
      "Epoch: 186, Loss: 18.4728\n",
      "Epoch: 187, Loss: 19.6173\n",
      "Epoch: 188, Loss: 23.4377\n",
      "Epoch: 189, Loss: 18.6714\n",
      "Epoch: 190, Loss: 22.1862\n",
      "Epoch: 191, Loss: 20.4641\n",
      "Epoch: 192, Loss: 24.6527\n",
      "Epoch: 193, Loss: 20.5589\n",
      "Epoch: 194, Loss: 25.4734\n",
      "Epoch: 195, Loss: 19.6381\n",
      "Epoch: 196, Loss: 17.6230\n",
      "Epoch: 197, Loss: 25.1486\n",
      "Epoch: 198, Loss: 19.1296\n",
      "Epoch: 199, Loss: 19.0811\n",
      "Epoch: 200, Loss: 23.0679\n",
      "Epoch: 201, Loss: 19.9878\n",
      "Epoch: 202, Loss: 19.7753\n",
      "Epoch: 203, Loss: 17.8086\n",
      "Epoch: 204, Loss: 18.9070\n",
      "Epoch: 205, Loss: 19.2614\n",
      "Epoch: 206, Loss: 24.3714\n",
      "Epoch: 207, Loss: 18.6739\n",
      "Epoch: 208, Loss: 19.1498\n",
      "Epoch: 209, Loss: 18.3278\n",
      "Epoch: 210, Loss: 17.7003\n",
      "Epoch: 211, Loss: 23.7862\n",
      "Epoch: 212, Loss: 17.8159\n",
      "Epoch: 213, Loss: 19.5974\n",
      "Epoch: 214, Loss: 18.2405\n",
      "Epoch: 215, Loss: 22.1990\n",
      "Epoch: 216, Loss: 19.4827\n",
      "Epoch: 217, Loss: 19.6033\n",
      "Epoch: 218, Loss: 19.0479\n",
      "Epoch: 219, Loss: 18.6483\n",
      "Epoch: 220, Loss: 18.3993\n",
      "Epoch: 221, Loss: 18.6526\n",
      "Epoch: 222, Loss: 22.2992\n",
      "Epoch: 223, Loss: 17.6946\n",
      "Epoch: 224, Loss: 18.6634\n",
      "Epoch: 225, Loss: 23.0253\n",
      "Epoch: 226, Loss: 18.6081\n",
      "Epoch: 227, Loss: 18.3143\n",
      "Epoch: 228, Loss: 18.5936\n",
      "Epoch: 229, Loss: 18.7599\n",
      "Epoch: 230, Loss: 18.1554\n",
      "Epoch: 231, Loss: 19.9232\n",
      "Epoch: 232, Loss: 16.7735\n",
      "Epoch: 233, Loss: 19.6772\n",
      "Epoch: 234, Loss: 20.0426\n",
      "Epoch: 235, Loss: 18.1808\n",
      "Epoch: 236, Loss: 24.6309\n",
      "Epoch: 237, Loss: 20.1354\n",
      "Epoch: 238, Loss: 19.3268\n",
      "Epoch: 239, Loss: 17.5088\n",
      "Epoch: 240, Loss: 18.3667\n",
      "Epoch: 241, Loss: 23.1320\n",
      "Epoch: 242, Loss: 22.5043\n",
      "Epoch: 243, Loss: 18.9734\n",
      "Epoch: 244, Loss: 17.8379\n",
      "Epoch: 245, Loss: 19.5147\n",
      "Epoch: 246, Loss: 18.6164\n",
      "Epoch: 247, Loss: 18.6602\n",
      "Epoch: 248, Loss: 17.7724\n",
      "Epoch: 249, Loss: 18.2656\n",
      "Epoch: 250, Loss: 17.7143\n",
      "Epoch: 251, Loss: 18.9665\n",
      "Epoch: 252, Loss: 20.6918\n",
      "Epoch: 253, Loss: 20.3352\n",
      "Epoch: 254, Loss: 17.9259\n",
      "Epoch: 255, Loss: 18.4163\n",
      "Epoch: 256, Loss: 17.9879\n",
      "Epoch: 257, Loss: 18.7950\n",
      "Epoch: 258, Loss: 17.8140\n",
      "Epoch: 259, Loss: 19.4936\n",
      "Epoch: 260, Loss: 22.4804\n",
      "Epoch: 261, Loss: 20.6548\n",
      "Epoch: 262, Loss: 18.9769\n",
      "Epoch: 263, Loss: 17.8422\n",
      "Epoch: 264, Loss: 19.3139\n",
      "Epoch: 265, Loss: 23.7852\n",
      "Epoch: 266, Loss: 21.0635\n",
      "Epoch: 267, Loss: 18.6130\n",
      "Epoch: 268, Loss: 19.5599\n",
      "Epoch: 269, Loss: 19.2775\n",
      "Epoch: 270, Loss: 17.5662\n",
      "Epoch: 271, Loss: 18.0544\n",
      "Epoch: 272, Loss: 16.4961\n",
      "Epoch: 273, Loss: 18.2638\n",
      "Epoch: 274, Loss: 18.3565\n",
      "Epoch: 275, Loss: 18.9090\n",
      "Epoch: 276, Loss: 17.9819\n",
      "Epoch: 277, Loss: 17.7777\n",
      "Epoch: 278, Loss: 17.1648\n",
      "Epoch: 279, Loss: 17.9959\n",
      "Epoch: 280, Loss: 17.2392\n",
      "Epoch: 281, Loss: 17.8072\n",
      "Epoch: 282, Loss: 17.5816\n",
      "Epoch: 283, Loss: 20.9049\n",
      "Epoch: 284, Loss: 20.6763\n",
      "Epoch: 285, Loss: 22.3133\n",
      "Epoch: 286, Loss: 17.2124\n",
      "Epoch: 287, Loss: 17.9240\n",
      "Epoch: 288, Loss: 19.4938\n",
      "Epoch: 289, Loss: 20.4019\n",
      "Epoch: 290, Loss: 19.4564\n",
      "Epoch: 291, Loss: 18.9085\n",
      "Epoch: 292, Loss: 17.6344\n",
      "Epoch: 293, Loss: 16.9060\n",
      "Epoch: 294, Loss: 19.6194\n",
      "Epoch: 295, Loss: 21.4325\n",
      "Epoch: 296, Loss: 16.8037\n",
      "Epoch: 297, Loss: 20.0972\n",
      "Epoch: 298, Loss: 17.1296\n",
      "Epoch: 299, Loss: 16.7781\n",
      "Epoch: 300, Loss: 17.5074\n",
      "Epoch: 301, Loss: 17.1450\n",
      "Epoch: 302, Loss: 16.8831\n",
      "Epoch: 303, Loss: 21.5088\n",
      "Epoch: 304, Loss: 16.0019\n",
      "Epoch: 305, Loss: 16.6502\n",
      "Epoch: 306, Loss: 17.1582\n",
      "Epoch: 307, Loss: 18.9091\n",
      "Epoch: 308, Loss: 20.3387\n",
      "Epoch: 309, Loss: 16.7365\n",
      "Epoch: 310, Loss: 16.8282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 311, Loss: 17.2527\n",
      "Epoch: 312, Loss: 16.6930\n",
      "Epoch: 313, Loss: 16.8078\n",
      "Epoch: 314, Loss: 16.4507\n",
      "Epoch: 315, Loss: 18.0079\n",
      "Epoch: 316, Loss: 16.7253\n",
      "Epoch: 317, Loss: 16.9656\n",
      "Epoch: 318, Loss: 17.2566\n",
      "Epoch: 319, Loss: 16.5008\n",
      "Epoch: 320, Loss: 16.9338\n",
      "Epoch: 321, Loss: 17.5072\n",
      "Epoch: 322, Loss: 16.6707\n",
      "Epoch: 323, Loss: 18.0343\n",
      "Epoch: 324, Loss: 16.9601\n",
      "Epoch: 325, Loss: 17.0829\n",
      "Epoch: 326, Loss: 17.4171\n",
      "Epoch: 327, Loss: 18.2696\n",
      "Epoch: 328, Loss: 19.4936\n",
      "Epoch: 329, Loss: 18.2931\n",
      "Epoch: 330, Loss: 19.6915\n",
      "Epoch: 331, Loss: 16.4829\n",
      "Epoch: 332, Loss: 17.9233\n",
      "Epoch: 333, Loss: 17.0456\n",
      "Epoch: 334, Loss: 18.4666\n",
      "Epoch: 335, Loss: 17.4295\n",
      "Epoch: 336, Loss: 20.2930\n",
      "Epoch: 337, Loss: 17.2180\n",
      "Epoch: 338, Loss: 18.1554\n",
      "Epoch: 339, Loss: 18.0699\n",
      "Epoch: 340, Loss: 18.4931\n",
      "Epoch: 341, Loss: 17.2587\n",
      "Epoch: 342, Loss: 17.7500\n",
      "Epoch: 343, Loss: 20.9068\n",
      "Epoch: 344, Loss: 16.4713\n",
      "Epoch: 345, Loss: 17.0282\n",
      "Epoch: 346, Loss: 16.9236\n",
      "Epoch: 347, Loss: 17.3426\n",
      "Epoch: 348, Loss: 16.7895\n",
      "Epoch: 349, Loss: 18.4562\n",
      "Epoch: 350, Loss: 16.3970\n",
      "Epoch: 351, Loss: 15.7124\n",
      "Epoch: 352, Loss: 16.6067\n",
      "Epoch: 353, Loss: 17.3796\n",
      "Epoch: 354, Loss: 16.7060\n",
      "Epoch: 355, Loss: 17.3664\n",
      "Epoch: 356, Loss: 16.8331\n",
      "Epoch: 357, Loss: 19.7261\n",
      "Epoch: 358, Loss: 19.1771\n",
      "Epoch: 359, Loss: 18.3921\n",
      "Epoch: 360, Loss: 16.2819\n",
      "Epoch: 361, Loss: 16.2071\n",
      "Epoch: 362, Loss: 16.3409\n",
      "Epoch: 363, Loss: 15.3154\n",
      "Epoch: 364, Loss: 16.6027\n",
      "Epoch: 365, Loss: 16.1922\n",
      "Epoch: 366, Loss: 16.9434\n",
      "Epoch: 367, Loss: 17.1912\n",
      "Epoch: 368, Loss: 16.2430\n",
      "Epoch: 369, Loss: 17.0348\n",
      "Epoch: 370, Loss: 16.7991\n",
      "Epoch: 371, Loss: 16.1461\n",
      "Epoch: 372, Loss: 15.6991\n",
      "Epoch: 373, Loss: 16.5122\n",
      "Epoch: 374, Loss: 16.8315\n",
      "Epoch: 375, Loss: 17.1035\n",
      "Epoch: 376, Loss: 15.9285\n",
      "Epoch: 377, Loss: 17.2811\n",
      "Epoch: 378, Loss: 15.6682\n",
      "Epoch: 379, Loss: 19.1271\n",
      "Epoch: 380, Loss: 16.3927\n",
      "Epoch: 381, Loss: 18.2248\n",
      "Epoch: 382, Loss: 15.1986\n",
      "Epoch: 383, Loss: 17.1247\n",
      "Epoch: 384, Loss: 17.4949\n",
      "Epoch: 385, Loss: 16.7192\n",
      "Epoch: 386, Loss: 17.2547\n",
      "Epoch: 387, Loss: 16.5843\n",
      "Epoch: 388, Loss: 16.1462\n",
      "Epoch: 389, Loss: 16.3598\n",
      "Epoch: 390, Loss: 19.1782\n",
      "Epoch: 391, Loss: 16.2802\n",
      "Epoch: 392, Loss: 16.3065\n",
      "Epoch: 393, Loss: 15.5944\n",
      "Epoch: 394, Loss: 16.0583\n",
      "Epoch: 395, Loss: 18.3801\n",
      "Epoch: 396, Loss: 18.2134\n",
      "Epoch: 397, Loss: 18.5899\n",
      "Epoch: 398, Loss: 17.8935\n",
      "Epoch: 399, Loss: 16.6455\n",
      "Epoch: 400, Loss: 17.5781\n",
      "Epoch: 401, Loss: 15.3874\n",
      "Epoch: 402, Loss: 17.3938\n",
      "Epoch: 403, Loss: 17.4868\n",
      "Epoch: 404, Loss: 18.0256\n",
      "Epoch: 405, Loss: 15.8284\n",
      "Epoch: 406, Loss: 18.5071\n",
      "Epoch: 407, Loss: 15.5396\n",
      "Epoch: 408, Loss: 16.9851\n",
      "Epoch: 409, Loss: 15.2876\n",
      "Epoch: 410, Loss: 16.1410\n",
      "Epoch: 411, Loss: 16.0188\n",
      "Epoch: 412, Loss: 17.6046\n",
      "Epoch: 413, Loss: 15.4197\n",
      "Epoch: 414, Loss: 15.2677\n",
      "Epoch: 415, Loss: 16.1328\n",
      "Epoch: 416, Loss: 16.7643\n",
      "Epoch: 417, Loss: 16.7231\n",
      "Epoch: 418, Loss: 17.3414\n",
      "Epoch: 419, Loss: 16.1777\n",
      "Epoch: 420, Loss: 16.0115\n",
      "Epoch: 421, Loss: 15.4862\n",
      "Epoch: 422, Loss: 16.5206\n",
      "Epoch: 423, Loss: 17.5758\n",
      "Epoch: 424, Loss: 15.8801\n",
      "Epoch: 425, Loss: 16.7826\n",
      "Epoch: 426, Loss: 18.9342\n",
      "Epoch: 427, Loss: 17.1423\n",
      "Epoch: 428, Loss: 16.2969\n",
      "Epoch: 429, Loss: 15.7584\n",
      "Epoch: 430, Loss: 16.7087\n",
      "Epoch: 431, Loss: 17.6749\n",
      "Epoch: 432, Loss: 15.4552\n",
      "Epoch: 433, Loss: 15.4022\n",
      "Epoch: 434, Loss: 17.0113\n",
      "Epoch: 435, Loss: 15.9034\n",
      "Epoch: 436, Loss: 18.6300\n",
      "Epoch: 437, Loss: 17.8773\n",
      "Epoch: 438, Loss: 15.0900\n",
      "Epoch: 439, Loss: 16.9024\n",
      "Epoch: 440, Loss: 17.4276\n",
      "Epoch: 441, Loss: 17.5240\n",
      "Epoch: 442, Loss: 18.7500\n",
      "Epoch: 443, Loss: 15.6346\n",
      "Epoch: 444, Loss: 15.0349\n",
      "Epoch: 445, Loss: 17.0379\n",
      "Epoch: 446, Loss: 18.3266\n",
      "Epoch: 447, Loss: 19.2981\n",
      "Epoch: 448, Loss: 17.8060\n",
      "Epoch: 449, Loss: 15.4637\n",
      "Epoch: 450, Loss: 15.9501\n",
      "Epoch: 451, Loss: 17.4647\n",
      "Epoch: 452, Loss: 16.0704\n",
      "Epoch: 453, Loss: 16.4588\n",
      "Epoch: 454, Loss: 15.2125\n",
      "Epoch: 455, Loss: 15.9750\n",
      "Epoch: 456, Loss: 17.4801\n",
      "Epoch: 457, Loss: 16.3204\n",
      "Epoch: 458, Loss: 15.5046\n",
      "Epoch: 459, Loss: 16.0088\n",
      "Epoch: 460, Loss: 14.9749\n",
      "Epoch: 461, Loss: 15.3998\n",
      "Epoch: 462, Loss: 15.5776\n",
      "Epoch: 463, Loss: 17.4111\n",
      "Epoch: 464, Loss: 16.7504\n",
      "Epoch: 465, Loss: 15.7190\n",
      "Epoch: 466, Loss: 15.2632\n",
      "Epoch: 467, Loss: 19.0555\n",
      "Epoch: 468, Loss: 15.4381\n",
      "Epoch: 469, Loss: 15.1225\n",
      "Epoch: 470, Loss: 14.8314\n",
      "Epoch: 471, Loss: 17.6008\n",
      "Epoch: 472, Loss: 16.8039\n",
      "Epoch: 473, Loss: 18.3234\n",
      "Epoch: 474, Loss: 15.6960\n",
      "Epoch: 475, Loss: 15.1778\n",
      "Epoch: 476, Loss: 15.3364\n",
      "Epoch: 477, Loss: 17.2020\n",
      "Epoch: 478, Loss: 15.4831\n",
      "Epoch: 479, Loss: 15.6433\n",
      "Epoch: 480, Loss: 16.7868\n",
      "Epoch: 481, Loss: 14.7846\n",
      "Epoch: 482, Loss: 15.2743\n",
      "Epoch: 483, Loss: 17.6035\n",
      "Epoch: 484, Loss: 15.1319\n",
      "Epoch: 485, Loss: 14.7814\n",
      "Epoch: 486, Loss: 16.1590\n",
      "Epoch: 487, Loss: 17.6137\n",
      "Epoch: 488, Loss: 17.6114\n",
      "Epoch: 489, Loss: 15.5344\n",
      "Epoch: 490, Loss: 16.4709\n",
      "Epoch: 491, Loss: 15.1075\n",
      "Epoch: 492, Loss: 16.4955\n",
      "Epoch: 493, Loss: 16.5427\n",
      "Epoch: 494, Loss: 15.3811\n",
      "Epoch: 495, Loss: 15.5601\n",
      "Epoch: 496, Loss: 15.4287\n",
      "Epoch: 497, Loss: 16.2977\n",
      "Epoch: 498, Loss: 16.2494\n",
      "Epoch: 499, Loss: 17.5950\n",
      "Epoch: 500, Loss: 15.2424\n",
      "Epoch: 501, Loss: 15.2275\n",
      "Epoch: 502, Loss: 17.0500\n",
      "Epoch: 503, Loss: 16.3484\n",
      "Epoch: 504, Loss: 15.8016\n",
      "Epoch: 505, Loss: 15.6430\n",
      "Epoch: 506, Loss: 17.6657\n",
      "Epoch: 507, Loss: 14.8082\n",
      "Epoch: 508, Loss: 15.4688\n",
      "Epoch: 509, Loss: 17.4016\n",
      "Epoch: 510, Loss: 14.7931\n",
      "Epoch: 511, Loss: 14.9791\n",
      "Epoch: 512, Loss: 14.9040\n",
      "Epoch: 513, Loss: 15.0407\n",
      "Epoch: 514, Loss: 17.9391\n",
      "Epoch: 515, Loss: 14.9327\n",
      "Epoch: 516, Loss: 15.4504\n",
      "Epoch: 517, Loss: 15.7527\n",
      "Epoch: 518, Loss: 17.1636\n",
      "Epoch: 519, Loss: 15.4285\n",
      "Epoch: 520, Loss: 14.9850\n",
      "Epoch: 521, Loss: 15.2745\n",
      "Epoch: 522, Loss: 16.6509\n",
      "Epoch: 523, Loss: 17.1440\n",
      "Epoch: 524, Loss: 17.4691\n",
      "Epoch: 525, Loss: 17.2181\n",
      "Epoch: 526, Loss: 14.9157\n",
      "Epoch: 527, Loss: 14.7039\n",
      "Epoch: 528, Loss: 15.5119\n",
      "Epoch: 529, Loss: 15.0293\n",
      "Epoch: 530, Loss: 16.8463\n",
      "Epoch: 531, Loss: 14.4638\n",
      "Epoch: 532, Loss: 15.2952\n",
      "Epoch: 533, Loss: 14.6253\n",
      "Epoch: 534, Loss: 15.4316\n",
      "Epoch: 535, Loss: 15.1543\n",
      "Epoch: 536, Loss: 14.9647\n",
      "Epoch: 537, Loss: 16.5439\n",
      "Epoch: 538, Loss: 16.6348\n",
      "Epoch: 539, Loss: 17.2494\n",
      "Epoch: 540, Loss: 15.0511\n",
      "Epoch: 541, Loss: 16.9448\n",
      "Epoch: 542, Loss: 15.5741\n",
      "Epoch: 543, Loss: 14.4498\n",
      "Epoch: 544, Loss: 14.9158\n",
      "Epoch: 545, Loss: 15.3043\n",
      "Epoch: 546, Loss: 16.0824\n",
      "Epoch: 547, Loss: 15.6011\n",
      "Epoch: 548, Loss: 14.7160\n",
      "Epoch: 549, Loss: 14.7738\n",
      "Epoch: 550, Loss: 14.2943\n",
      "Epoch: 551, Loss: 14.3416\n",
      "Epoch: 552, Loss: 16.7325\n",
      "Epoch: 553, Loss: 14.5005\n",
      "Epoch: 554, Loss: 16.6612\n",
      "Epoch: 555, Loss: 16.3735\n",
      "Epoch: 556, Loss: 15.5203\n",
      "Epoch: 557, Loss: 15.2966\n",
      "Epoch: 558, Loss: 16.0996\n",
      "Epoch: 559, Loss: 15.2993\n",
      "Epoch: 560, Loss: 14.8159\n",
      "Epoch: 561, Loss: 14.9763\n",
      "Epoch: 562, Loss: 15.1347\n",
      "Epoch: 563, Loss: 14.4268\n",
      "Epoch: 564, Loss: 15.5409\n",
      "Epoch: 565, Loss: 14.5843\n",
      "Epoch: 566, Loss: 14.7124\n",
      "Epoch: 567, Loss: 15.3027\n",
      "Epoch: 568, Loss: 15.4318\n",
      "Epoch: 569, Loss: 15.5252\n",
      "Epoch: 570, Loss: 14.9257\n",
      "Epoch: 571, Loss: 17.0434\n",
      "Epoch: 572, Loss: 14.6104\n",
      "Epoch: 573, Loss: 16.7363\n",
      "Epoch: 574, Loss: 14.9385\n",
      "Epoch: 575, Loss: 16.3026\n",
      "Epoch: 576, Loss: 16.3100\n",
      "Epoch: 577, Loss: 15.4668\n",
      "Epoch: 578, Loss: 14.7660\n",
      "Epoch: 579, Loss: 15.4423\n",
      "Epoch: 580, Loss: 14.7591\n",
      "Epoch: 581, Loss: 14.9355\n",
      "Epoch: 582, Loss: 14.7325\n",
      "Epoch: 583, Loss: 14.8305\n",
      "Epoch: 584, Loss: 15.0070\n",
      "Epoch: 585, Loss: 15.8376\n",
      "Epoch: 586, Loss: 14.6564\n",
      "Epoch: 587, Loss: 14.1227\n",
      "Epoch: 588, Loss: 15.1866\n",
      "Epoch: 589, Loss: 15.2849\n",
      "Epoch: 590, Loss: 14.3788\n",
      "Epoch: 591, Loss: 13.9116\n",
      "Epoch: 592, Loss: 16.3763\n",
      "Epoch: 593, Loss: 16.3296\n",
      "Epoch: 594, Loss: 14.5962\n",
      "Epoch: 595, Loss: 14.7934\n",
      "Epoch: 596, Loss: 14.2839\n",
      "Epoch: 597, Loss: 15.0622\n",
      "Epoch: 598, Loss: 14.9146\n",
      "Epoch: 599, Loss: 14.5150\n",
      "Epoch: 600, Loss: 15.3013\n",
      "Epoch: 601, Loss: 14.4796\n",
      "Epoch: 602, Loss: 14.7536\n",
      "Epoch: 603, Loss: 15.7757\n",
      "Epoch: 604, Loss: 15.1143\n",
      "Epoch: 605, Loss: 14.5348\n",
      "Epoch: 606, Loss: 16.2819\n",
      "Epoch: 607, Loss: 14.5718\n",
      "Epoch: 608, Loss: 14.2946\n",
      "Epoch: 609, Loss: 14.4322\n",
      "Epoch: 610, Loss: 15.9497\n",
      "Epoch: 611, Loss: 14.4534\n",
      "Epoch: 612, Loss: 14.6556\n",
      "Epoch: 613, Loss: 14.8765\n",
      "Epoch: 614, Loss: 14.2177\n",
      "Epoch: 615, Loss: 15.1701\n",
      "Epoch: 616, Loss: 14.6022\n",
      "Epoch: 617, Loss: 15.5364\n",
      "Epoch: 618, Loss: 14.7429\n",
      "Epoch: 619, Loss: 14.2449\n",
      "Epoch: 620, Loss: 14.4942\n",
      "Epoch: 621, Loss: 15.4325\n",
      "Epoch: 622, Loss: 14.0484\n",
      "Epoch: 623, Loss: 14.7431\n",
      "Epoch: 624, Loss: 14.3202\n",
      "Epoch: 625, Loss: 14.7149\n",
      "Epoch: 626, Loss: 14.5046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 627, Loss: 15.2282\n",
      "Epoch: 628, Loss: 15.5121\n",
      "Epoch: 629, Loss: 15.3692\n",
      "Epoch: 630, Loss: 14.7111\n",
      "Epoch: 631, Loss: 15.7877\n",
      "Epoch: 632, Loss: 14.2672\n",
      "Epoch: 633, Loss: 14.3564\n",
      "Epoch: 634, Loss: 14.5108\n",
      "Epoch: 635, Loss: 14.5512\n",
      "Epoch: 636, Loss: 14.3260\n",
      "Epoch: 637, Loss: 14.5227\n",
      "Epoch: 638, Loss: 14.3771\n",
      "Epoch: 639, Loss: 15.4766\n",
      "Epoch: 640, Loss: 14.5591\n",
      "Epoch: 641, Loss: 14.2827\n",
      "Epoch: 642, Loss: 14.2680\n",
      "Epoch: 643, Loss: 14.1354\n",
      "Epoch: 644, Loss: 14.1406\n",
      "Epoch: 645, Loss: 15.9227\n",
      "Epoch: 646, Loss: 14.0058\n",
      "Epoch: 647, Loss: 14.5606\n",
      "Epoch: 648, Loss: 16.0014\n",
      "Epoch: 649, Loss: 14.5924\n",
      "Epoch: 650, Loss: 14.5462\n",
      "Epoch: 651, Loss: 14.9046\n",
      "Epoch: 652, Loss: 14.9974\n",
      "Epoch: 653, Loss: 14.4154\n",
      "Epoch: 654, Loss: 14.6372\n",
      "Epoch: 655, Loss: 14.9878\n",
      "Epoch: 656, Loss: 14.2594\n",
      "Epoch: 657, Loss: 15.7935\n",
      "Epoch: 658, Loss: 14.3940\n",
      "Epoch: 659, Loss: 13.9196\n",
      "Epoch: 660, Loss: 15.5542\n",
      "Epoch: 661, Loss: 14.2000\n",
      "Epoch: 662, Loss: 16.5487\n",
      "Epoch: 663, Loss: 14.7009\n",
      "Epoch: 664, Loss: 14.3643\n",
      "Epoch: 665, Loss: 14.8466\n",
      "Epoch: 666, Loss: 14.2213\n",
      "Epoch: 667, Loss: 14.4815\n",
      "Epoch: 668, Loss: 14.1112\n",
      "Epoch: 669, Loss: 15.6998\n",
      "Epoch: 670, Loss: 14.0274\n",
      "Epoch: 671, Loss: 14.7518\n",
      "Epoch: 672, Loss: 14.1506\n",
      "Epoch: 673, Loss: 13.9883\n",
      "Epoch: 674, Loss: 15.5077\n",
      "Epoch: 675, Loss: 14.2403\n",
      "Epoch: 676, Loss: 14.5484\n",
      "Epoch: 677, Loss: 14.0975\n",
      "Epoch: 678, Loss: 14.6036\n",
      "Epoch: 679, Loss: 14.6333\n",
      "Epoch: 680, Loss: 14.2047\n",
      "Epoch: 681, Loss: 14.2187\n",
      "Epoch: 682, Loss: 14.0945\n",
      "Epoch: 683, Loss: 14.5095\n",
      "Epoch: 684, Loss: 15.9571\n",
      "Epoch: 685, Loss: 14.0082\n",
      "Epoch: 686, Loss: 13.6029\n",
      "Epoch: 687, Loss: 14.4333\n",
      "Epoch: 688, Loss: 14.0491\n",
      "Epoch: 689, Loss: 14.1405\n",
      "Epoch: 690, Loss: 13.9743\n",
      "Epoch: 691, Loss: 14.4463\n",
      "Epoch: 692, Loss: 14.4500\n",
      "Epoch: 693, Loss: 15.1717\n",
      "Epoch: 694, Loss: 14.4706\n",
      "Epoch: 695, Loss: 14.2146\n",
      "Epoch: 696, Loss: 14.0977\n",
      "Epoch: 697, Loss: 13.5721\n",
      "Epoch: 698, Loss: 14.6153\n",
      "Epoch: 699, Loss: 13.9933\n",
      "Epoch: 700, Loss: 14.1396\n",
      "Epoch: 701, Loss: 14.5170\n",
      "Epoch: 702, Loss: 13.8464\n",
      "Epoch: 703, Loss: 15.2127\n",
      "Epoch: 704, Loss: 14.5586\n",
      "Epoch: 705, Loss: 15.4483\n",
      "Epoch: 706, Loss: 15.4679\n",
      "Epoch: 707, Loss: 15.1532\n",
      "Epoch: 708, Loss: 14.1752\n",
      "Epoch: 709, Loss: 14.4795\n",
      "Epoch: 710, Loss: 13.5901\n",
      "Epoch: 711, Loss: 13.7383\n",
      "Epoch: 712, Loss: 14.9346\n",
      "Epoch: 713, Loss: 14.8967\n",
      "Epoch: 714, Loss: 14.9991\n",
      "Epoch: 715, Loss: 13.8636\n",
      "Epoch: 716, Loss: 15.0760\n",
      "Epoch: 717, Loss: 14.4605\n",
      "Epoch: 718, Loss: 15.2859\n",
      "Epoch: 719, Loss: 14.2497\n",
      "Epoch: 720, Loss: 14.9974\n",
      "Epoch: 721, Loss: 13.8135\n",
      "Epoch: 722, Loss: 14.3236\n",
      "Epoch: 723, Loss: 14.0669\n",
      "Epoch: 724, Loss: 14.0381\n",
      "Epoch: 725, Loss: 14.4686\n",
      "Epoch: 726, Loss: 14.8613\n",
      "Epoch: 727, Loss: 13.8098\n",
      "Epoch: 728, Loss: 15.0032\n",
      "Epoch: 729, Loss: 14.0889\n",
      "Epoch: 730, Loss: 15.2332\n",
      "Epoch: 731, Loss: 13.5615\n",
      "Epoch: 732, Loss: 15.0250\n",
      "Epoch: 733, Loss: 13.7561\n",
      "Epoch: 734, Loss: 13.9860\n",
      "Epoch: 735, Loss: 14.3737\n",
      "Epoch: 736, Loss: 13.5894\n",
      "Epoch: 737, Loss: 14.5371\n",
      "Epoch: 738, Loss: 13.5760\n",
      "Epoch: 739, Loss: 13.7798\n",
      "Epoch: 740, Loss: 13.8108\n",
      "Epoch: 741, Loss: 13.7656\n",
      "Epoch: 742, Loss: 14.9365\n",
      "Epoch: 743, Loss: 13.5190\n",
      "Epoch: 744, Loss: 14.8396\n",
      "Epoch: 745, Loss: 13.9320\n",
      "Epoch: 746, Loss: 14.1334\n",
      "Epoch: 747, Loss: 14.0131\n",
      "Epoch: 748, Loss: 15.2019\n",
      "Epoch: 749, Loss: 13.5741\n",
      "Epoch: 750, Loss: 13.9781\n",
      "Epoch: 751, Loss: 13.9316\n",
      "Epoch: 752, Loss: 15.0355\n",
      "Epoch: 753, Loss: 15.0756\n",
      "Epoch: 754, Loss: 14.1865\n",
      "Epoch: 755, Loss: 14.1919\n",
      "Epoch: 756, Loss: 14.0515\n",
      "Epoch: 757, Loss: 13.9579\n",
      "Epoch: 758, Loss: 13.5989\n",
      "Epoch: 759, Loss: 14.1018\n",
      "Epoch: 760, Loss: 13.8389\n",
      "Epoch: 761, Loss: 13.7847\n",
      "Epoch: 762, Loss: 13.8878\n",
      "Epoch: 763, Loss: 13.6614\n",
      "Epoch: 764, Loss: 13.9681\n",
      "Epoch: 765, Loss: 14.9451\n",
      "Epoch: 766, Loss: 14.8114\n",
      "Epoch: 767, Loss: 14.0641\n",
      "Epoch: 768, Loss: 14.0114\n",
      "Epoch: 769, Loss: 14.9486\n",
      "Epoch: 770, Loss: 13.9070\n",
      "Epoch: 771, Loss: 13.7113\n",
      "Epoch: 772, Loss: 14.2958\n",
      "Epoch: 773, Loss: 13.6216\n",
      "Epoch: 774, Loss: 14.7722\n",
      "Epoch: 775, Loss: 14.0474\n",
      "Epoch: 776, Loss: 13.6877\n",
      "Epoch: 777, Loss: 13.3679\n",
      "Epoch: 778, Loss: 14.0008\n",
      "Epoch: 779, Loss: 13.9095\n",
      "Epoch: 780, Loss: 13.9210\n",
      "Epoch: 781, Loss: 13.8475\n",
      "Epoch: 782, Loss: 14.0182\n",
      "Epoch: 783, Loss: 14.3786\n",
      "Epoch: 784, Loss: 14.0751\n",
      "Epoch: 785, Loss: 13.4140\n",
      "Epoch: 786, Loss: 13.6061\n",
      "Epoch: 787, Loss: 13.2575\n",
      "Epoch: 788, Loss: 14.6952\n",
      "Epoch: 789, Loss: 13.6762\n",
      "Epoch: 790, Loss: 13.5238\n",
      "Epoch: 791, Loss: 14.4794\n",
      "Epoch: 792, Loss: 14.0462\n",
      "Epoch: 793, Loss: 13.6563\n",
      "Epoch: 794, Loss: 13.6520\n",
      "Epoch: 795, Loss: 14.6028\n",
      "Epoch: 796, Loss: 14.5004\n",
      "Epoch: 797, Loss: 13.7997\n",
      "Epoch: 798, Loss: 13.7659\n",
      "Epoch: 799, Loss: 13.9353\n",
      "Epoch: 800, Loss: 13.8977\n",
      "Epoch: 801, Loss: 14.4806\n",
      "Epoch: 802, Loss: 13.6958\n",
      "Epoch: 803, Loss: 13.6005\n",
      "Epoch: 804, Loss: 13.4637\n",
      "Epoch: 805, Loss: 13.4555\n",
      "Epoch: 806, Loss: 14.3788\n",
      "Epoch: 807, Loss: 14.1712\n",
      "Epoch: 808, Loss: 14.3817\n",
      "Epoch: 809, Loss: 13.8702\n",
      "Epoch: 810, Loss: 13.6254\n",
      "Epoch: 811, Loss: 13.4992\n",
      "Epoch: 812, Loss: 13.7302\n",
      "Epoch: 813, Loss: 14.1985\n",
      "Epoch: 814, Loss: 13.5739\n",
      "Epoch: 815, Loss: 13.1283\n",
      "Epoch: 816, Loss: 13.2810\n",
      "Epoch: 817, Loss: 13.6310\n",
      "Epoch: 818, Loss: 13.6163\n",
      "Epoch: 819, Loss: 13.1302\n",
      "Epoch: 820, Loss: 14.6712\n",
      "Epoch: 821, Loss: 13.4569\n",
      "Epoch: 822, Loss: 13.8169\n",
      "Epoch: 823, Loss: 13.5641\n",
      "Epoch: 824, Loss: 14.7366\n",
      "Epoch: 825, Loss: 14.4488\n",
      "Epoch: 826, Loss: 13.6360\n",
      "Epoch: 827, Loss: 13.6787\n",
      "Epoch: 828, Loss: 13.7627\n",
      "Epoch: 829, Loss: 14.5901\n",
      "Epoch: 830, Loss: 14.2373\n",
      "Epoch: 831, Loss: 13.7840\n",
      "Epoch: 832, Loss: 13.6750\n",
      "Epoch: 833, Loss: 13.9485\n",
      "Epoch: 834, Loss: 13.4472\n",
      "Epoch: 835, Loss: 14.0350\n",
      "Epoch: 836, Loss: 14.3457\n",
      "Epoch: 837, Loss: 13.4475\n",
      "Epoch: 838, Loss: 13.9962\n",
      "Epoch: 839, Loss: 14.3492\n",
      "Epoch: 840, Loss: 14.5266\n",
      "Epoch: 841, Loss: 13.7791\n",
      "Epoch: 842, Loss: 13.6986\n",
      "Epoch: 843, Loss: 14.2481\n",
      "Epoch: 844, Loss: 13.7541\n",
      "Epoch: 845, Loss: 13.2373\n",
      "Epoch: 846, Loss: 13.2037\n",
      "Epoch: 847, Loss: 14.6371\n",
      "Epoch: 848, Loss: 14.2809\n",
      "Epoch: 849, Loss: 13.2679\n",
      "Epoch: 850, Loss: 13.8565\n",
      "Epoch: 851, Loss: 13.4676\n",
      "Epoch: 852, Loss: 13.3118\n",
      "Epoch: 853, Loss: 13.5031\n",
      "Epoch: 854, Loss: 13.1860\n",
      "Epoch: 855, Loss: 13.9242\n",
      "Epoch: 856, Loss: 13.8476\n",
      "Epoch: 857, Loss: 14.2006\n",
      "Epoch: 858, Loss: 14.3322\n",
      "Epoch: 859, Loss: 13.3533\n",
      "Epoch: 860, Loss: 13.2867\n",
      "Epoch: 861, Loss: 13.1957\n",
      "Epoch: 862, Loss: 13.3925\n",
      "Epoch: 863, Loss: 13.6892\n",
      "Epoch: 864, Loss: 13.3957\n",
      "Epoch: 865, Loss: 13.2627\n",
      "Epoch: 866, Loss: 13.1967\n",
      "Epoch: 867, Loss: 13.4594\n",
      "Epoch: 868, Loss: 13.0883\n",
      "Epoch: 869, Loss: 13.3598\n",
      "Epoch: 870, Loss: 13.0288\n",
      "Epoch: 871, Loss: 13.4204\n",
      "Epoch: 872, Loss: 13.2477\n",
      "Epoch: 873, Loss: 13.7086\n",
      "Epoch: 874, Loss: 13.0540\n",
      "Epoch: 875, Loss: 13.2780\n",
      "Epoch: 876, Loss: 12.9905\n"
     ]
    }
   ],
   "source": [
    "model = GCN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out[data_long.x - 1], data.y[data_long.x - 1])  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss\n",
    "\n",
    "for epoch in range(1, 1001):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
